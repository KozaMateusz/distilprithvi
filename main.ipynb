{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mkoza/workspace/ml/distilprithvi/venv/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "/home/mkoza/workspace/ml/distilprithvi/venv/lib/python3.12/site-packages/lightning/pytorch/cli.py:530: LightningCLI's args parameter is intended to run from within Python like if it were from the command line. To prevent mistakes it is not recommended to provide both args and command line arguments, got: sys.argv[1:]=['--f=/run/user/1003/jupyter/runtime/kernel-v35416a706037c785073752ad43753c4b91fdc7500.json'], args=['--config', 'teachers/hls_burn_scars_teacher/burn_scars_config.yaml'].\n",
      "Seed set to 2\n",
      "INFO:root:Loaded weights for HLSBands.BLUE in position 0 of patch embed\n",
      "INFO:root:Loaded weights for HLSBands.GREEN in position 1 of patch embed\n",
      "INFO:root:Loaded weights for HLSBands.RED in position 2 of patch embed\n",
      "INFO:root:Loaded weights for HLSBands.NIR_NARROW in position 3 of patch embed\n",
      "INFO:root:Loaded weights for HLSBands.SWIR_1 in position 4 of patch embed\n",
      "INFO:root:Loaded weights for HLSBands.SWIR_2 in position 5 of patch embed\n",
      "WARNING:root:Decoder UNetDecoder does not have an `includes_head` attribute. Falling back to the value of the registry.\n",
      "/home/mkoza/workspace/ml/distilprithvi/venv/lib/python3.12/site-packages/lightning/pytorch/cli.py:683: `SemanticSegmentationTask.configure_optimizers` will be overridden by `MyLightningCLI.configure_optimizers`.\n",
      "Using bfloat16 Automatic Mixed Precision (AMP)\n",
      "You are using the plain ModelCheckpoint callback. Consider using LitModelCheckpoint which with seamless uploading to Model registry.\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "You are using the plain ModelCheckpoint callback. Consider using LitModelCheckpoint which with seamless uploading to Model registry.\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name         | Type                     | Params | Mode \n",
      "------------------------------------------------------------------\n",
      "0 | teacher      | SemanticSegmentationTask | 324 M  | eval \n",
      "1 | student      | LRASPPMobileNetV3Large   | 3.2 M  | train\n",
      "2 | kd_criterion | KLDivLoss                | 0      | train\n",
      "------------------------------------------------------------------\n",
      "327 M     Trainable params\n",
      "0         Non-trainable params\n",
      "327 M     Total params\n",
      "1,309.694 Total estimated model params size (MB)\n",
      "262       Modules in train mode\n",
      "618       Modules in eval mode\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                                           "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mkoza/workspace/ml/distilprithvi/venv/lib/python3.12/site-packages/lightning/pytorch/utilities/data.py:79: Trying to infer the `batch_size` from an ambiguous collection. The batch size we found is 16. To avoid any miscalculations, use `self.log(..., batch_size=batch_size)`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mkoza/workspace/ml/distilprithvi/venv/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:310: The number of training batches (32) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0:   3%|â–Ž         | 1/32 [00:01<00:33,  0.91it/s, v_num=5d64]"
     ]
    }
   ],
   "source": [
    "from terratorch.cli_tools import LightningInferenceModel\n",
    "from distillers.semantic_segmentation_distiller import SemanticSegmentationDistiller\n",
    "from students.deeplabv3_mobilenet_v3_large import DeepLabV3MobileNetV3Large\n",
    "from students.lraspp_mobilenet_v3_large import LRASPPMobileNetV3Large\n",
    "import torch\n",
    "from lightning import Trainer\n",
    "from lightning.pytorch.loggers import MLFlowLogger\n",
    "\n",
    "torch.set_float32_matmul_precision('medium')\n",
    "\n",
    "TEACHER_CONFIG = \"teachers/hls_burn_scars_teacher/burn_scars_config.yaml\"\n",
    "TEACHER_CHECKPOINT = \"teachers/hls_burn_scars_teacher/Prithvi_EO_V2_300M_BurnScars.pt\"\n",
    "STUDENT_MODEL = \"lraspp\"  # or \"deeplabv3\"\n",
    "BATCH_SIZE = 16\n",
    "NUM_EPOCHS = 50\n",
    "EXPERIMENT_NAME = \"hls_burn_scars_distillation\"\n",
    "RUN_NAME = \"hls_burn_scars_distillation_run\"\n",
    "KD_TEMPERATURE = 4.0\n",
    "KD_WEIGHT = 0.75\n",
    "\n",
    "\n",
    "inference_model = LightningInferenceModel.from_config(\n",
    "    TEACHER_CONFIG, TEACHER_CHECKPOINT\n",
    ")\n",
    "teacher = inference_model.model\n",
    "datamodule = inference_model.datamodule\n",
    "datamodule.batch_size = BATCH_SIZE\n",
    "\n",
    "if STUDENT_MODEL == \"deeplabv3\":\n",
    "    student = DeepLabV3MobileNetV3Large(\n",
    "        num_channels=len(datamodule.output_bands),\n",
    "        num_classes=datamodule.num_classes,\n",
    "    )\n",
    "elif STUDENT_MODEL == \"lraspp\":\n",
    "    student = LRASPPMobileNetV3Large(\n",
    "        num_channels=len(datamodule.output_bands),\n",
    "        num_classes=datamodule.num_classes,\n",
    "    )\n",
    "\n",
    "distiller = SemanticSegmentationDistiller(\n",
    "    teacher=teacher,\n",
    "    student=student,\n",
    "    kd_temperature=KD_TEMPERATURE,\n",
    "    kd_weight=KD_WEIGHT,\n",
    ")\n",
    "\n",
    "mlf_logger = MLFlowLogger(experiment_name=EXPERIMENT_NAME, run_name=RUN_NAME)\n",
    "trainer = Trainer(max_epochs=NUM_EPOCHS, logger=mlf_logger)\n",
    "trainer.fit(distiller, datamodule)\n",
    "trainer.test(distiller, datamodule)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
