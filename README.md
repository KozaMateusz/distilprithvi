# Distil Prithvi

Knowledge distillation of geospatial foundation models.

## Datasets & Teachers
Datasets and teachers can be obtained on [huggingface](https://huggingface.co/collections/KozaMateusz/distil-prithvi-680ca48149d5d8a9ad3d25e3).

## TODO
* self.aug is used probably only for inference as dataloader should apply everything when training
* add the following metrics: IoU, mIoU
* add distillation
* check the floods dataset