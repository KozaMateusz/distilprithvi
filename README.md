# Distil Prithvi

Knowledge distillation of geospatial foundation models.

## Datasets & Teachers
Datasets and teachers can be obtained on [huggingface](https://huggingface.co/collections/KozaMateusz/distil-prithvi-680ca48149d5d8a9ad3d25e3).

## TODO
* determine why performance on the test dataset is different when running the same model (teacher vs distilprithvi using trainer.fit)
* add distillation
* check the floods dataset